import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
from sklearn.metrics import f1_score
from .dataset_drone import load_split
from .model_drone import DroneClassifier
from .config import LR, WEIGHT_DECAY, EPOCHS, BATCH_SIZE, NUM_WORKERS, DEVICE
import os

# ======================
# âš™ï¸ ë°ì´í„°ì…‹ ë¡œë“œ
# ======================
train_ds, val_ds, test_ds = load_split(
    balance_neg_to_pos=True,   # ğŸ‘ˆ ë“œë¡  ìˆ˜ë§Œí¼ ë¹„ë“œë¡  ì–¸ë”ìƒ˜í”Œë§
    # max_neg=2000,            # ğŸ‘ˆ (ì˜µì…˜) ë¹„ë“œë¡ ì„ ìµœëŒ€ 2000ê°œë¡œ ì œí•œ
)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)
val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)
device = DEVICE

# ======================
# âš™ï¸ ëª¨ë¸ ì •ì˜
# ======================
model = DroneClassifier().to(device)

# ======================
# âš™ï¸ pos_weight ìë™ ê³„ì‚°
# ======================
def count_labels(subset):
    cnt_pos = cnt_neg = 0
    for idx in subset.indices:
        _, lbl = subset.dataset.data[idx]
        if lbl == 1:
            cnt_pos += 1
        else:
            cnt_neg += 1
    return cnt_pos, cnt_neg

n_pos, n_neg = count_labels(train_ds)
ratio = (n_neg / max(n_pos, 1)) if n_pos > 0 else 1.0
print(f"[ClassRatio] pos={n_pos}, neg={n_neg}, pos_weight={ratio:.3f}")

pos_weight = torch.tensor([1.5], device=device)
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

# ======================
# âš™ï¸ í•™ìŠµ ë£¨í”„
# ======================
best_f1 = 0.0
os.makedirs("chk", exist_ok=True)       # âœ… chk í´ë” ìë™ ìƒì„±
save_path = "chk/best_model.pt"         # âœ… í‰ê°€ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ì„¤ì •

for epoch in range(1, EPOCHS + 1):
    model.train()
    running_loss = 0.0
    for x, y in tqdm(train_loader, desc=f"[Epoch {epoch}/{EPOCHS}] Train"):
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        y_pred = model(x)
        loss = criterion(y_pred.squeeze(), y)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    avg_loss = running_loss / len(train_loader)

    # ======================
    # âš™ï¸ Validation
    # ======================
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for x, y in val_loader:
            x, y = x.to(device), y.to(device)
            out = torch.sigmoid(model(x)).squeeze()
            preds.extend((out > 0.3).int().cpu().numpy())
            trues.extend(y.int().cpu().numpy())

    val_f1 = f1_score(trues, preds, zero_division=0)
    print(f"[Epoch {epoch}/{EPOCHS}] loss={avg_loss:.4f} | val_F1={val_f1:.4f}")

    # âœ… F1 ìµœê³ ê°’ì¼ ë•Œë§Œ ì €ì¥
    if val_f1 > best_f1:
        best_f1 = val_f1
        torch.save(model.state_dict(), save_path)
        print(f"  âœ… New best model saved (F1={val_f1:.4f})")

print(f"\nğŸ¯ Training complete! Best F1={best_f1:.4f} | Saved: {save_path}")

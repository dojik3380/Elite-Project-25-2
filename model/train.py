# model/train.py
import os, torch
from torch.utils.data import DataLoader
from sklearn.metrics import f1_score
from collections import defaultdict
from tqdm import tqdm
import numpy as np

from model.dataset_drone import load_split, SEGMENT_SEC   # ðŸ”¹ SEGMENT_SEC ì¶”ê°€
from model.model_drone import DroneMultiClassifier
from model.config import EPOCHS, BATCH_SIZE, LR, WEIGHT_DECAY, TARGET_SR
from model.augment import WaveAugmentV1   # ðŸ”¹ ì˜¨ë¼ì¸ ì¦ê°•ìš©

DEVICE=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# ============================================================
# âœ… ë°ì´í„° ë¡œë“œ
# ============================================================
train_ds, val_ds, test_ds, label_map = load_split()
n_classes = len(label_map)
print(f"\nðŸ“‚ Loaded dataset with {n_classes} classes:")
for k, v in label_map.items():
    print(f"  {v}: {k}")

if len(train_ds) == 0:
    raise RuntimeError("ðŸš¨ Train dataset is empty!")

# ============================================================
# âœ… í´ëž˜ìŠ¤ ë¶„í¬ ë° ê°€ì¤‘ì¹˜ ê³„ì‚° (ë£¨íŠ¸ ìŠ¤ì¼€ì¼ë§, duration ê¸°ë°˜)
#    â†’ DroneDatasetëŠ” ì´ë¯¸ 2ì´ˆ segmentë¡œ ìª¼ê°œì ¸ ìžˆìœ¼ë¯€ë¡œ
#      ê° segment í•˜ë‚˜ë‹¹ SEGMENT_SEC(=WIN_SEC) ì´ˆë¡œ ê³„ì‚°
# ============================================================
duration_sum = defaultdict(float)

print("\nâ³ Calculating class durations (segment-based) ...")

for i in range(len(train_ds)):
    _, label = train_ds[i]          # train_ds[i] = (x, y)
    if torch.is_tensor(label):
        label_idx = int(label.item())
    else:
        label_idx = int(label)
    duration_sum[label_idx] += SEGMENT_SEC   # segment í•˜ë‚˜ = SEGMENT_SEC ì´ˆ

# weight = 1 / sqrt(total_seconds)
class_weights_list = []
for cls_idx in range(n_classes):
    total_sec = duration_sum[cls_idx]
    if total_sec == 0:
        w = 1.0
    else:
        w = 1.0 / np.sqrt(total_sec)
    class_weights_list.append(w)

class_weights = torch.tensor(class_weights_list, dtype=torch.float).to(DEVICE)

print("ðŸŽš Class weights (duration-based, segment-level):")
for cls, w in zip(label_map.keys(), class_weights_list):
    print(f"  {cls:12s}: {w:.6f}")

# ============================================================
# âœ… DataLoader
# ============================================================
train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# ============================================================
# âœ… Augment (ì˜¨ë¼ì¸ ì¦ê°•, ë…¸ì´ì¦ˆ OFF)
# ============================================================
# use_noise=False â†’ real noise ì¦ê°• ë¹„í™œì„±í™”, ë‚˜ë¨¸ì§€ gain/shift/stretch/time_maskë§Œ ì‚¬ìš©
augment = WaveAugmentV1(
    sample_rate=TARGET_SR,
    use_augment=True,   # gain/shift/stretch/time_mask í™œì„±í™”
    use_noise=False     # noise ì¦ê°• ë¹„í™œì„±í™”
)


# ============================================================
# âœ… ëª¨ë¸ / ì†ì‹¤í•¨ìˆ˜ / ì˜µí‹°ë§ˆì´ì €
# ============================================================
model = DroneMultiClassifier(n_classes=n_classes).to(DEVICE)
criterion = torch.nn.CrossEntropyLoss(weight=class_weights)
optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='max', factor=0.5, patience=3,
)

best_f1 = 0.0
os.makedirs("chk", exist_ok=True)

# ============================================================
# âœ… í•™ìŠµ ë£¨í”„
# ============================================================
for epoch in range(1, EPOCHS + 1):

    # -----------------------
    # ðŸ”µ TRAIN
    # -----------------------
    model.train()
    running_loss = 0.0
    progress = tqdm(enumerate(train_loader), total=len(train_loader), ncols=100, colour='cyan')
    progress.set_description(f"[Epoch {epoch:02d}]")

    for batch_idx, (x, y) in progress:
        x, y = x.to(DEVICE), y.to(DEVICE)

        # ðŸ”¥ ì˜¨ë¼ì¸ ì¦ê°• (ë…¸ì´ì¦ˆ OFF, batch ì•ˆ ê° ì„¸ê·¸ë¨¼íŠ¸ì— ë…ë¦½ì ìœ¼ë¡œ ì ìš©)
        with torch.no_grad():
            B = x.size(0)
            x_aug = torch.empty_like(x)
            for i in range(B):
                x_aug[i] = augment(x[i])
            x = x_aug

        optimizer.zero_grad()
        logits = model(x)
        loss = criterion(logits, y)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        avg_loss = running_loss / (batch_idx + 1)
        progress.set_postfix(loss=f"{avg_loss:.4f}")

    # -----------------------
    # ðŸ”µ VALIDATION (ì¦ê°• ì—†ìŒ)
    # -----------------------
    model.eval()
    y_true, y_pred = [], []

    with torch.no_grad():
        for x, y in val_loader:
            x = x.to(DEVICE)
            pred = torch.argmax(model(x), dim=1)

            y_true.extend(y.cpu().numpy())
            y_pred.extend(pred.cpu().numpy())

    f1 = f1_score(y_true, y_pred, average="macro")
    scheduler.step(f1)

    current_lr = optimizer.param_groups[0]['lr']
    print(f"\n[{epoch:02d}] Epoch complete | avg_loss={avg_loss:.4f} | val_f1={f1:.4f} | lr={current_lr:.2e}")

    if f1 > best_f1:
        best_f1 = f1
        torch.save({"model": model.state_dict(), "label_map": label_map}, "chk/best.pt")
        print(f"  âœ… New best model saved (F1={best_f1:.4f})")

print("\nðŸŽ‰ Training complete.")
print("Final label map:", label_map)
